{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING IMAGES FROM GOOGLE\n",
    "\n",
    "Came across two methods:\n",
    "1. Using Beautiful Soup\n",
    "2. Using Selenium\n",
    "\n",
    "Got stuck on Beautiful Soup so had to switch to Selenium. I found Selenium a bit more complex than Beautiful Soup, but I think it's visual output feature makes it a fascinating tool.\n",
    "\n",
    "The Selenium package does what a human user would normally do on the web browser. In this case I want to go to Google Images and search for images of Dogs and store it on my desktop, Selenium would automate the process for us by specifying which image you want to download and how many images you want to download. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chrome Driver Path\n",
    "\n",
    "To use Selenium with Google Chrome we need to download a Chrome Driver, depending on the Google Chrome Version the Chrome Driver is installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH =  '/Users/apurvasalvi/Desktop/GauguinBot/chromedriver'\n",
    "wd = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "wd.get('https://google.com') \n",
    "search_box = wd.find_element_by_css_selector('input.gLFyf') #input box selector\n",
    "search_box.send_keys('dogs')\n",
    "wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above lines of code only opens the browser and gives the input query and quits.\n",
    "\n",
    "The second phase would involve to search for the query, go to the image section and get the respective image links using css selectors. \n",
    "\n",
    "The third phase of the code will be to download the images from the link onto your local computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Web Scraping from Google Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_urls(plot:str, max_links_to_fetch:int, wd:webdriver):\n",
    "    # build the google plot\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "    wd.get(search_url.format(q=plot))\n",
    "    image_urls = set() #used so duplicates won't be added\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    while image_count < max_links_to_fetch:\n",
    "        #find elements based on the tag and class name using css selector\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\") \n",
    "        number_links = len(thumbnail_results)\n",
    "        print(f\"Found: {number_links} search results. Extracting links from {results_start}:{number_links}\")\n",
    "        for img in thumbnail_results[results_start:number_links]:\n",
    "            #try to click every thumbnail such that we can get the real image behind it\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(1)\n",
    "            except Exception:\n",
    "                continue\n",
    "            #extract image urls    \n",
    "            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "            image_count = len(image_urls)\n",
    "            if len(image_urls) >= max_links_to_fetch:\n",
    "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Found:\", len(image_urls), \"image links only\")\n",
    "            time.sleep(30)\n",
    "            return\n",
    "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "            if load_more_button:\n",
    "                wd.execute_script(\"document.plotSelector('.mye4qd').click();\")\n",
    "        results_start = len(thumbnail_results)\n",
    "    return image_urls\n",
    "\n",
    "def download_image(folder_path:str,file_name:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "        \n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')  #Opens and identifies the given image file\n",
    "        folder_path = os.path.join(folder_path,file_name) #Joins 2 or more pathname components\n",
    "        if os.path.exists(folder_path):\n",
    "            #if the path exists, add file to the folder path\n",
    "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        else:\n",
    "            #else create a new folder and add file to the new folder\n",
    "            os.mkdir(folder_path)\n",
    "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:  #'wb': mode for binary random access, opens and truncates the file to 0 bytes\n",
    "            image.save(f, \"JPEG\", quality=85)  \n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wd = webdriver.Chrome(executable_path=DRIVER_PATH)  #controls chrome driver and allows you to drive the browser\n",
    "    plot_names = [\"cats\"]  #list of search keywords\n",
    "    for plot in plot_names:\n",
    "        wd.get('https://google.com')   #loads webpage in the current browser session\n",
    "        search_box = wd.find_element_by_css_selector('input.gLFyf')   #finds an element by css selector and returns it if found\n",
    "        search_box.send_keys(plot)   #simulates typing into the element\n",
    "        links = fetch_image_urls(plot,10,wd)  #gets image urls\n",
    "        images_path = '/Users/apurvasalvi/Desktop/GauguinBot/images' #folder to save the element\n",
    "        for i in links:\n",
    "            download_image(images_path,plot,i)   #downloads images to the specified path\n",
    "    wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "\n",
    "External Source: 70%\n",
    "Personal Contribution: 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "1. Article title:\tWeb Scraping Images from Google\n",
    "\n",
    "   Website title:\tMedium\n",
    "   \n",
    "   URL          :\thttps://medium.com/@wwwanandsuresh/web-scraping-images-from-google-9084545808a2\n",
    "   \n",
    "   \n",
    "2. Article title:\tMsalmannasir/Google_image_scraper\n",
    "\n",
    "   Website title:\tGitHub\n",
    "   \n",
    "   URL          :\thttps://github.com/Msalmannasir/Google_image_scraper/blob/master/google_img."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This task helped me learn how to use Selenium to web scrape any image from the internet. The task is divided into 3 steps: Opening the Web Browser using Selenium, Getting the URLs, and Downloading the images using these URLs. The above lines of codes can be used to download any number of images from the internet. Thus, I have automated the process of getting images from the web using Selenium. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
